{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "3_models_4004ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOklxJuSmtxKWvxX59Uwgwg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhileswar-Komati/Suicide_Ideation/blob/master/3_models_4004ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq7NNI4r8phF",
        "outputId": "2c56778e-c50e-4fb1-eefe-599d45463c0b"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from collections import defaultdict\n",
        "np.seterr(divide = 'ignore')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEq6mzmy8tiY"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "2GDYD69G812r",
        "outputId": "5eb6417b-3662-4155-e683-01d1184f1fc6"
      },
      "source": [
        "data = pd.read_csv('/content/Suicide_Watch_PRAW_4004.csv')\n",
        "data.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>id</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>New wiki on how to avoid accidentally encourag...</td>\n",
              "      <td>cz6nfd</td>\n",
              "      <td>SuicideWatch</td>\n",
              "      <td>We've been seeing a worrying increase in pro-s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Please remember that NO ACTIVISM of any kind i...</td>\n",
              "      <td>iq0w21</td>\n",
              "      <td>SuicideWatch</td>\n",
              "      <td>Activism, i.e. advocating or fundraising for s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Failed suicide = Expensive</td>\n",
              "      <td>kajy93</td>\n",
              "      <td>SuicideWatch</td>\n",
              "      <td>I recently made a suicide attempt. Unfortunate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>My cat is the only reason I'm still alive.</td>\n",
              "      <td>kaz14g</td>\n",
              "      <td>SuicideWatch</td>\n",
              "      <td>Let's start saying that I'm Italian so my engl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Being alive is pointless (makes no sense)</td>\n",
              "      <td>kay9t0</td>\n",
              "      <td>SuicideWatch</td>\n",
              "      <td>Let's say you're mentally healthy and not suic...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...                                               body\n",
              "0  New wiki on how to avoid accidentally encourag...  ...  We've been seeing a worrying increase in pro-s...\n",
              "1  Please remember that NO ACTIVISM of any kind i...  ...  Activism, i.e. advocating or fundraising for s...\n",
              "2                         Failed suicide = Expensive  ...  I recently made a suicide attempt. Unfortunate...\n",
              "3         My cat is the only reason I'm still alive.  ...  Let's start saying that I'm Italian so my engl...\n",
              "4          Being alive is pointless (makes no sense)  ...  Let's say you're mentally healthy and not suic...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp2RWgC89KTA",
        "outputId": "7a92e4b4-fd52-4743-8042-43f275df5379"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4004, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WmEe6Kb9eSs",
        "outputId": "4405f28f-5eaa-4dba-f120-19d4e421ac7f"
      },
      "source": [
        "data['subreddit'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SuicideWatch    2018\n",
              "depression      1986\n",
              "Name: subreddit, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "e9RnB91sVcLS",
        "outputId": "9793f928-a17e-496f-b1ea-538353b9cc4f"
      },
      "source": [
        "data['body'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'We\\'ve been seeing a worrying increase in pro-suicide content showing up here and, and also going unreported. This undermines our purpose here, so we wanted to highlight and clarify our guidelines about both direct and indirect incitement of suicide.  \\n\\nWe\\'ve created a wiki that covers these issues.  We hope this will be helpful to anyone who\\'s wondering whether something\\'s okay here and which responses to report.  It explains in detail why *any* validation of suicidal intent, even an \"innocent\" message like \"if you\\'re 100% committed, I\\'ll just wish you peace\" is likely to increase people\\'s pain, and why it\\'s important to report even subtle pro-suicide comments. The full text of the wiki\\'s current version is below, and it is maintained at [/r/SuicideWatch/wiki/incitement](http://www.reddit.com/r/SuicideWatch/wiki/incitement). \\n\\nWe deeply appreciate everyone who gives responsive, empathetic, non-judgemental support to our OPs, and we particularly thank everyone who\\'s already been reporting incitement in all forms.  \\n\\nPlease report any post or comment that encourages suicide (or that breaks any of the other guidelines in the sidebar) to the moderators, either by clicking the \"report\" button or by [sending us a modmail](https://www.reddit.com/message/compose?to=%2Fr%2FSuicideWatch) with a link. We deal with all guideline violations that are reported to us as soon as we can, but we can\\'t read everything so community reports are essential. If you get a PM that breaks the guidelines, please report it both [to the reddit sitewide admins](http://www.reddit.com/report) and to us in modmail. \\n\\nThanks to all the great citizens of the community who help flag problem content and behaviour for us.  \\n    \\n    \\n\\n******\\n***[/r/SuicideWatch/wiki/incitement](http://www.reddit.com/r/SuicideWatch/wiki/incitement)***  \\n*******\\n\\n###Summary###\\n\\n**It\\'s important to respect and understand people\\'s experiences and emotions. It\\'s never necessary, helpful, or kind to support suicidal intent. There are some common misconceptions (discussed below) about suicidal people and how to help them that can cause well-meaning people to inadvertently incite suicide. There are also people online who incite suicide on purpose, often while pretending to be sympathetic and helpful.** \\n\\n###Validate Feelings and Experiences, Not Self-Destructive Intentions###\\n\\nWe\\'re here to offer support, not judgement.  That means accepting, with the best understanding we can offer, whatever emotions people express.  Suicidal people are suffering, and we\\'re here to try to ease that by providing support and caring.  The most reliable way we know to de-escalate someone at risk is to give them the experience of feeling understood. That means not judging whether they should be feeling the way they are, or telling them what to do or not do.  \\n\\nBut there\\'s an important line to draw here.  There\\'s a crucial difference between empathizing with feelings and responding non-judgmentally to suicidal thoughts, and in any way endorsing, encouraging, or validating suicidal intentions or hopeless beliefs. **It\\'s both possible and important to convey understanding and compassion for someone\\'s suicidal thoughts without putting your finger on the scale of their decision.**\\n\\nAnything that condones suicide, even passively, *encourages* suicide. It isn\\'t supportive and does not help. It also violates reddit\\'s sitewide rules as well as our guidelines. Explicitly inciting suicide online is a criminal offense in most jurisdictions. \\n\\nDo not treat any OP\\'s post as meaning that will definitely die by suicide and can\\'t change their minds or be helped. Anyone who\\'s able to read the comments here still has a chance to choose whether or not to try to keep living, even if they\\'ve also been experiencing intense thoughts of suicide, made a suicide plan, or started carrying it out.  \\n\\nIn [the most useful empirical model we have](https://www.apa.org/science/about/psa/2009/06/sci-brief), the desire to die by suicide primarily comes from two interpersonal factors; alienation and a sense of being a burden or having nothing to offer. These factors usually lead to a profound feeling of being unwelcome in the world. \\n\\n**So, any acceptance or reinforcement of suicidal intent, even something \"innocent\" like \"I hope you find peace\", is actually a form of covert shunning that validates a person\\'s sense that they\\'re unwelcome in the world. It will usually add to their pain even if kindly meant and gently worded.**  \\n\\n###How to Avoid Validating Suicidal Intent###\\n\\nKeep the following in mind when offering support to anyone at risk for suicide.  \\n\\n* **People who say they don\\'t want help usually can feel better if they get support that doesn\\'t invalidate their emotions.** Unfortunately, [many popular \"good\" responses are actually counterproductive](https://www.speakingofsuicide.com/2015/03/03/what-not-to-say/). In particular, many friends and family tend to rely exclusively on trying to convince the suicidal person that \"it\\'s not so bad\", and this is usually experienced as \"I don\\'t understand what you\\'re going through and I\\'m not going to try\".  People who\\'ve had \"help\" that made them feel worse don\\'t want any more of the same.  It doesn\\'t mean that someone who actually knows how to be supportive can\\'t give them any comfort.   \\n\\n*  **Most people who are suicidal want to end their** ***pain,*** **not their lives.**  It\\'s almost never true that death is the only way to end these people\\'s suffering. Of course there are exceptional situations, and we certainly acknowledge that, for some people, the right help can be difficult to find. But preventing someone\\'s suicide doesn\\'t mean prolonging their suffering if we do it by giving them real comfort and understanding. \\n\\n* ***An unfixable problem doesn\\'t mean that a good life will never be possible***.  We don\\'t have to fix or change anything to help someone feel better. It\\'s important to keep in mind that the correlation between our outer circumstances and our inner experience is weaker and less direct than commonly assumed.  For every kind of difficult life situation, you will find some people who lapse into suicidal despair, and others who cope amazingly well, and a whole spectrum in between. A key difference is how much inner resilience the person has at the time. This can depend on many personal and situational factors. But when there\\'s not enough, interpersonal support can both compensate for its absence and help rebuild it.  We go into more depth on the \"it gets better\" issue in [this PSA Post](https://www.reddit.com/r/SuicideWatch/comments/25igd7/whats_wrong_with_it_gets_better_what_if_it_doesnt/) which is always linked from our sidebar (community info on mobile) guidelines. \\n\\n* **There are** ***always*** **more choices than brutally forcing someone to stay alive or passively letting them end their lives**.\\n\\nTo avoid accidentally breaking the anti-incitement rule, don\\'t say or try to imply that acting on suicidal thoughts is a good idea, or that someone can\\'t turn back or is already dead.  Do whatever you can to help them feel cared for and welcome, at least in this little corner of the world.  [Our talking tips](http://redd.it/igh87) offer more detailed guidance.\\n\\n###Look Out for Deliberate Incitement.  It May Come in Disguise.###\\n\\nOften comments that subtly encourage suicidal intent actually come from suicide fetishists and voyeurs ([unfortunately this is a real and disturbing phenomenon](https://en.wikipedia.org/wiki/William_Francis_Melchert-Dinkel)). People like this *are* out there and the anonymous nature of reddit makes us particularly attractive to them.  \\n\\nThey will typically try to scratch their psychological \"itch\" by saying things that push people closer to the edge.  They often do this by exploiting the myths that we debunked in the bullet points above.  Specifically you might see people doing the following:\\n\\n* Encouraging the false belief that the only way suicidal people can end their pain is by dying.  **There are** ***always*** **more and better choices than \"brutally forcing someone to stay alive\" or helping (actively or passively) them to end their lives**.\\n\\n* Creating an artificial and toxic sense of \"solidarity\" by linking their encouragement of suicide to empathy.  They will represent themselves as the only one who really understand the suicidal person, while either directly or indirectly encouraging their self-loathing emotions and self-destructive impulses.  **Since most people in suicidal crisis are in desperate need to empathy and understanding, this is a particularly dangerous form of manipulation.**  \\n\\nMany suicide inciters are adept at putting a benevolent spin on their activities while actually luring people away from sources of real help.  A couple of key points to keep in mind: \\n\\n* **Skilled suicide intervention -- peer or professional -- is based on empathic responsiveness to the person\\'s feelings that reduces their suffering in the moment.**  Contrary to pop-culture myths, it does **not** involve persuasion (\"Don\\'t do it!\"), cheerleading (\"You\\'ve got this!\") or meaningless false promises (\"Trust me, it gets better!\"), or invalidation (\"Let me show you how things aren\\'t as bad as you think!\").  Anyone who leads others to expect these kinds of toxic responses, or any other response that prolongs their pain, from expert help may be covertly pro-suicide. (Of course, people sometimes do have bad experience when seeking mental-health treatment, and it\\'s fine to vent about those, but processing our own disappointment and frustration is entirely different from trying to destroy someone else\\'s hope of getting help.)\\n\\n* **Choices made by competent responders are always informed by the understanding that breaching someone\\'s trust is traumatic and must be avoided if possible.**  Any kind of involuntary intervention is an **extremely unlikely** outcome when someone consults a clinician or calls a hotline. (Confidentiality is addressed in more detail in [our Hotlines FAQ post](http://redd.it/1c7ntr)). The goal is always to provide all help with the client\\'s full knowledge and informed consent. We know that no individual or system is perfect.  Mistakes that lead to bad experiences do sometimes happen to vulnerable people, and we have enormous sympathy for them. But anyone who suggests that this is the norm might be trying to scare people away from the help they need.  \\n\\nPlease [let us know discreetly](https://www.reddit.com/message/compose?to=%2Fr%2FSuicideWatch) if you see anyone exhibiting these or similar behaviours. We don\\'t recommend trying to engage with them directly.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqUjwC2WQt_x",
        "outputId": "fb83327e-9cc1-4123-b19c-985b0b790ef2"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH4C7cjw-UcX",
        "outputId": "b4c64ff1-b542-456b-a191-c55f071a3f3d"
      },
      "source": [
        "text = \"Hello, Stupid:: idiot!\"\n",
        "def preprocess(string):\n",
        "  string = str(string)\n",
        "  phrase = re.sub(r\"n\\'t\", \" not\", string)\n",
        "  phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "  phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "  phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "  phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "  phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "  phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "  phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "  phrase = re.sub('[^a-z0-9]+', ' ', phrase, flags=re.IGNORECASE)\n",
        "  phrase = re.sub('(\\s+)', ' ', phrase)\n",
        "  phrase = phrase.lower()\n",
        "\n",
        "  text_tokens = word_tokenize(phrase)\n",
        "  return \" \".join([word for word in text_tokens if not word in stopwords.words()])\n",
        "print(preprocess(data['body'][0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seeing worrying increase pro suicide content showing going unreported undermines purpose wanted highlight clarify guidelines direct indirect incitement suicide created wiki covers issues hope helpful anyone wondering whether something okay responses report explains detail validation suicidal intent even innocent message like 100 committed wish peace likely increase people pain important report even subtle pro suicide comments full text wiki current version maintained r suicidewatch wiki incitement http www reddit r suicidewatch wiki incitement deeply appreciate everyone gives responsive empathetic judgemental support ops particularly thank everyone already reporting incitement forms please report post comment encourages suicide breaks guidelines sidebar moderators either clicking report button sending us modmail https www reddit message compose 2fr 2fsuicidewatch link deal guideline violations reported us soon read everything community reports essential get pm breaks guidelines please report reddit sitewide admins http www reddit report us modmail thanks great citizens community help flag problem content behaviour us r suicidewatch wiki incitement http www reddit r suicidewatch wiki incitement summary important respect understand people experiences emotions never necessary helpful kind support suicidal intent common misconceptions discussed suicidal people help cause well meaning people inadvertently incite suicide people online incite suicide purpose often pretending sympathetic helpful validate feelings experiences self destructive intentions offer support judgement means accepting best understanding offer whatever emotions people express suicidal people suffering try ease providing support caring reliable way know escalate someone risk give experience feeling understood means judging whether feeling way telling important line draw crucial difference empathizing feelings responding judgmentally suicidal thoughts way endorsing encouraging validating suicidal intentions hopeless beliefs possible important convey understanding compassion someone suicidal thoughts without putting finger scale decision anything condones suicide even passively encourages suicide supportive help violates reddit sitewide rules well guidelines explicitly inciting suicide online criminal offense jurisdictions treat post meaning definitely suicide change minds helped anyone able read comments still chance choose whether try keep living even experiencing intense thoughts suicide made suicide plan started carrying useful empirical model https www org science psa 2009 06 sci brief desire suicide primarily comes two interpersonal factors alienation sense burden nothing offer factors usually lead profound feeling unwelcome world acceptance reinforcement suicidal intent even something innocent like hope find peace actually form covert shunning validates person sense unwelcome world usually add pain even kindly meant gently worded avoid validating suicidal intent keep following mind offering support anyone risk suicide people say help usually feel better get support invalidate emotions unfortunately many popular good responses actually counterproductive https www speakingofsuicide 2015 03 03 say particular many friends family tend rely exclusively trying convince suicidal person bad usually experienced understand going going try people help made feel worse mean someone actually knows supportive give comfort people suicidal pain lives almost never true death way people suffering course exceptional situations certainly acknowledge people right help difficult find preventing someone suicide mean prolonging suffering giving real comfort understanding unfixable problem mean good life never possible fix change anything help someone feel better important keep mind correlation outer circumstances inner experience weaker less direct commonly assumed every kind difficult life situation find people lapse suicidal despair others cope amazingly well whole spectrum key difference much inner resilience person time depend many personal situational factors enough interpersonal support compensate absence help rebuild go depth gets better issue psa post https www reddit r suicidewatch comments 25igd7 whats wrong gets better doesnt always linked sidebar community info mobile guidelines always choices brutally forcing someone stay alive passively letting lives avoid accidentally breaking anti incitement rule say try imply acting suicidal thoughts good idea someone turn back already dead whatever help feel cared welcome least little corner world talking tips http redd igh87 offer detailed guidance look deliberate incitement may disguise often comments subtly encourage suicidal intent actually suicide fetishists voyeurs unfortunately real disturbing phenomenon https wikipedia org wiki william francis melchert dinkel people like anonymous nature reddit makes us particularly attractive typically try scratch psychological itch saying things push people closer edge often exploiting myths debunked bullet points specifically might see people following encouraging false belief way suicidal people pain dying always better choices brutally forcing someone stay alive helping actively passively lives creating artificial toxic sense solidarity linking encouragement suicide empathy represent really understand suicidal person either directly indirectly encouraging self loathing emotions self destructive impulses since people suicidal crisis desperate need empathy understanding particularly dangerous form manipulation many suicide inciters adept putting benevolent spin activities actually luring people away sources real help couple key points keep mind skilled suicide intervention peer professional based empathic responsiveness person feelings reduces suffering moment contrary pop culture myths involve persuasion cheerleading got meaningless false promises trust gets better invalidation let show things bad think anyone leads others expect kinds toxic responses response prolongs pain expert help may covertly pro suicide course people sometimes bad experience seeking mental health treatment fine vent processing disappointment frustration entirely different trying destroy someone else hope getting help choices made competent responders always informed understanding breaching someone trust traumatic must avoided possible kind involuntary intervention extremely unlikely outcome someone consults clinician calls hotline confidentiality addressed detail hotlines faq post http redd 1c7ntr goal always provide help client full knowledge informed consent know individual system perfect mistakes lead bad experiences sometimes happen vulnerable people enormous sympathy anyone suggests norm might trying scare people away help need please let us know discreetly https www reddit message compose 2fr 2fsuicidewatch see anyone exhibiting similar behaviours recommend trying engage directly\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-oZAD2gRdRK"
      },
      "source": [
        "data['text'] = data['title'] + data['body']\n",
        "data['text'] = data['text'].map(lambda string: preprocess(string))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf25MgqBYzG2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtQHLJRMhfX-"
      },
      "source": [
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3nx1AhfhpCb",
        "outputId": "0840b36d-63d2-4c58-80a1-f81e1e9ebc9d"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install bert-for-tf2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 22.4MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 29.7MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 23.9MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 19.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 13.6MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61kB 14.3MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 14.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 14.3MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92kB 14.0MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 13.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112kB 13.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122kB 13.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133kB 13.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143kB 13.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153kB 13.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 174kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 245kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 276kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 675kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 686kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 706kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 716kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 747kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 757kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 788kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 819kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 829kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 849kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 860kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 880kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 890kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 901kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 911kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 921kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 931kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 962kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 983kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 993kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0MB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0MB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0MB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0MB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1MB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1MB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1MB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1MB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 13.6MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n",
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/d3/820ccaf55f1e24b5dd43583ac0da6d86c2d27bbdfffadbba69bafe73ca93/bert-for-tf2-0.14.7.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.7MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.7-cp36-none-any.whl size=30537 sha256=0a4458936a905d4c9f9abee75453b0f5752f977c62d94f71c1d23a588c163939\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/f8/e2/b98f79a6b8cc898d8e4102b83acb8a098df7d27500a2bac912\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=93b01ac0da0b772ea7c78690658afdf3c8159749be02680ffed18ff511d3d79f\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19475 sha256=f5f19b17da3fd3d05c4b2b0bcb5714fd691103883b2a4ae088fe2e49f1e43dde\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.7 params-flow-0.8.2 py-params-0.9.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9wDuSy2hkvu",
        "outputId": "a8a6c822-e236-4ac1-d21f-ee62dd4ac41a"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import bert\n",
        "import tokenization\n",
        "module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
        "bert_layer = hub.KerasLayer(module_url, trainable=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:resolver HttpCompressedFileResolver does not support the provided handle.\n",
            "INFO:absl:resolver GcsCompressedFileResolver does not support the provided handle.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnV7yhrgV-dh",
        "outputId": "d760c09a-59f7-43b5-829f-b9bb65416e7b"
      },
      "source": [
        "X = data['text'].values\n",
        "y = data['subreddit'].values\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "y = le.fit_transform(y)\n",
        "# X = list(X)\n",
        "# y = list(y)\n",
        "print(set(y))\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.02, random_state = 42)\n",
        "print(len(train_X), len(test_X))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0, 1}\n",
            "3923 81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uNH2_0UhzRv"
      },
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "\n",
        "def bert_encode(texts, tokenizer, max_len=512):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    \n",
        "    for text in texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "            \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len - len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "    \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewFF3fa3iwd2"
      },
      "source": [
        "from keras.regularizers import l2\n",
        "def build_model(bert_layer, max_len=512):\n",
        "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    net = tf.keras.layers.Dense(64, activation='relu', activity_regularizer = l2(0.001))(clf_output)\n",
        "    net = tf.keras.layers.Dropout(0.5)(net)\n",
        "    net = tf.keras.layers.Dense(32, activation='relu')(net)\n",
        "    net = tf.keras.layers.Dropout(0.5)(net)\n",
        "    out = tf.keras.layers.Dense(1, activation='sigmoid')(net)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "    model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_YFGuzWi77P"
      },
      "source": [
        "import keras\n",
        "max_len = 200\n",
        "train_input = bert_encode(train_X, tokenizer, max_len = max_len)\n",
        "test_input = bert_encode(test_X, tokenizer, max_len = max_len)\n",
        "train_labels = train_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYlvA6SUjYAI",
        "outputId": "6ecba8fa-ae2d-4c03-a69c-432043a2e68b"
      },
      "source": [
        "model = build_model(bert_layer, max_len=max_len)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_17\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        multiple             109482241   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_8 (Te [(None, 768)]        0           keras_layer[8][1]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 64)           49216       tf_op_layer_strided_slice_8[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 64)           0           dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 32)           2080        dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 32)           0           dense_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 1)            33          dropout_7[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 109,533,570\n",
            "Trainable params: 109,533,569\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqU0rp5Uly7g",
        "outputId": "e649a174-b2d8-4c6c-8592-a8c941975e6a"
      },
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "\n",
        "train_history = model.fit(\n",
        "    train_input, train_labels, \n",
        "    validation_split = 0.02,\n",
        "    epochs = 5,\n",
        "    callbacks = [checkpoint, earlystopping],\n",
        "    batch_size = 8,\n",
        "    verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "481/481 [==============================] - ETA: 0s - loss: 0.7034 - accuracy: 0.5047\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.43038, saving model to model.h5\n",
            "481/481 [==============================] - 235s 488ms/step - loss: 0.7034 - accuracy: 0.5047 - val_loss: 0.6954 - val_accuracy: 0.4304\n",
            "Epoch 2/5\n",
            "481/481 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5029\n",
            "Epoch 00002: val_accuracy did not improve from 0.43038\n",
            "481/481 [==============================] - 229s 476ms/step - loss: 0.6932 - accuracy: 0.5029 - val_loss: 0.6953 - val_accuracy: 0.4304\n",
            "Epoch 3/5\n",
            "481/481 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5044\n",
            "Epoch 00003: val_accuracy did not improve from 0.43038\n",
            "481/481 [==============================] - 229s 476ms/step - loss: 0.6933 - accuracy: 0.5044 - val_loss: 0.6944 - val_accuracy: 0.4304\n",
            "Epoch 4/5\n",
            "481/481 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5047\n",
            "Epoch 00004: val_accuracy did not improve from 0.43038\n",
            "481/481 [==============================] - 229s 476ms/step - loss: 0.6933 - accuracy: 0.5047 - val_loss: 0.6941 - val_accuracy: 0.4304\n",
            "Epoch 5/5\n",
            "481/481 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4987\n",
            "Epoch 00005: val_accuracy did not improve from 0.43038\n",
            "481/481 [==============================] - 229s 476ms/step - loss: 0.6932 - accuracy: 0.4987 - val_loss: 0.6944 - val_accuracy: 0.4304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wEL_4vwl7Wp"
      },
      "source": [
        "model.load_weights('model.h5')\n",
        "test_pred = model.predict(test_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muj98zx0klCd"
      },
      "source": [
        "def my_round(val):\n",
        "  if val >= 0.5:\n",
        "    return 1\n",
        "  return 0\n",
        "test_pred = [my_round(ele) for ele in test_pred]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IeTKX7ancQA",
        "outputId": "334583e8-fcef-48cd-b9ec-05a02b7e038b"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "acc = accuracy_score(test_y, test_pred)\n",
        "print (\"Test Set Examples: \", len(test_y)) \n",
        "print (\"Test Set Accuracy: \", acc * 100, \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Set Examples:  81\n",
            "Test Set Accuracy:  79.01234567901234 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujbznuv2b4kU",
        "outputId": "bb7814a4-28e8-443d-8f41-f3d2ae5e3812"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBClassifier\n",
        "clf = Pipeline([('cv', CountVectorizer()), ('xgb', XGBClassifier())])\n",
        "\n",
        "clf.fit(train_X, train_y)\n",
        "print(\"------------Training Done ----------\")\n",
        "predictions = clf.predict(test_X)\n",
        "\n",
        "test_acc_sklearn = np.sum(predictions == test_y) / float(len(test_y)) \n",
        "\n",
        "\n",
        "print (\"Test Set Examples: \", len(test_y)) \n",
        "print (\"Test Set Accuracy: \", test_acc_sklearn * 100, \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------Training Done ----------\n",
            "Test Set Examples:  81\n",
            "Test Set Accuracy:  72.8395061728395 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4A0Wrgwev6E"
      },
      "source": [
        "vocab_size = 10000\n",
        "embedding_dim = 50\n",
        "max_length = 100\n",
        "oov_tok = '<OOV>'\n",
        "trunc_type = 'post'\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
        "tokenizer.fit_on_texts(train_X)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(train_X)\n",
        "padded = pad_sequences(sequences, maxlen = max_length, truncating = trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(test_X)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen = max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZABqMPQh9SK",
        "outputId": "9096c9ec-bf17-4c85-a031-6d73491028d2"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 50)           500000    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 5000)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                160032    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 660,065\n",
            "Trainable params: 660,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5_g5fxtiBpA",
        "outputId": "9b6d137f-00ee-46a2-bbb8-23f811f5f6a6"
      },
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model2.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "\n",
        "num_epochs = 10\n",
        "model.fit(padded, train_y, \n",
        "          epochs = num_epochs, \n",
        "          validation_data = (testing_padded, test_y),\n",
        "          callbacks = [checkpoint, earlystopping],\n",
        "          batch_size = 16,\n",
        "          verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "242/246 [============================>.] - ETA: 0s - loss: 0.6720 - accuracy: 0.5723\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.71605, saving model to model2.h5\n",
            "246/246 [==============================] - 2s 7ms/step - loss: 0.6709 - accuracy: 0.5741 - val_loss: 0.5919 - val_accuracy: 0.7160\n",
            "Epoch 2/10\n",
            "245/246 [============================>.] - ETA: 0s - loss: 0.3386 - accuracy: 0.8589\n",
            "Epoch 00002: val_accuracy did not improve from 0.71605\n",
            "246/246 [==============================] - 2s 7ms/step - loss: 0.3385 - accuracy: 0.8590 - val_loss: 0.8360 - val_accuracy: 0.5679\n",
            "Epoch 3/10\n",
            "242/246 [============================>.] - ETA: 0s - loss: 0.1005 - accuracy: 0.9610\n",
            "Epoch 00003: val_accuracy did not improve from 0.71605\n",
            "246/246 [==============================] - 1s 6ms/step - loss: 0.1003 - accuracy: 0.9610 - val_loss: 1.0418 - val_accuracy: 0.5432\n",
            "Epoch 4/10\n",
            "243/246 [============================>.] - ETA: 0s - loss: 0.0629 - accuracy: 0.9712\n",
            "Epoch 00004: val_accuracy did not improve from 0.71605\n",
            "246/246 [==============================] - 2s 6ms/step - loss: 0.0638 - accuracy: 0.9709 - val_loss: 1.2186 - val_accuracy: 0.5309\n",
            "Epoch 5/10\n",
            "243/246 [============================>.] - ETA: 0s - loss: 0.0557 - accuracy: 0.9727\n",
            "Epoch 00005: val_accuracy did not improve from 0.71605\n",
            "246/246 [==============================] - 2s 7ms/step - loss: 0.0557 - accuracy: 0.9725 - val_loss: 1.3481 - val_accuracy: 0.5062\n",
            "Epoch 6/10\n",
            "241/246 [============================>.] - ETA: 0s - loss: 0.0541 - accuracy: 0.9720\n",
            "Epoch 00006: val_accuracy did not improve from 0.71605\n",
            "246/246 [==============================] - 2s 6ms/step - loss: 0.0535 - accuracy: 0.9725 - val_loss: 1.4583 - val_accuracy: 0.4938\n",
            "Epoch 00006: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd30f094eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75IVxWuRjfeZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}