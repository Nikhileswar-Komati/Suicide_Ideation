{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_models_3032.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOm32Zi/VnecyLeSNSx+ano",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhileswar-Komati/Suicide_Ideation/blob/master/3_models_3032.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq7NNI4r8phF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f766616-189d-4462-de34-4c09bc152fd9"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from collections import defaultdict\n",
        "np.seterr(divide = 'ignore')"
      ],
      "execution_count": 394,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'divide': 'ignore', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 394
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEq6mzmy8tiY"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GDYD69G812r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "82096231-cad1-4df3-f7ea-4f3ee10ce776"
      },
      "source": [
        "data = pd.read_csv('/content/Suicide_Watch_PRAW_3032.csv')\n",
        "data.head(5)"
      ],
      "execution_count": 396,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>id</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>New wiki on how to avoid accidentally encourag...</td>\n",
              "      <td>cz6nfd</td>\n",
              "      <td>SuicideWatch</td>\n",
              "      <td>We've been seeing a worrying increase in pro-s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Please remember that NO ACTIVISM of any kind i...</td>\n",
              "      <td>iq0w21</td>\n",
              "      <td>SuicideWatch</td>\n",
              "      <td>Activism, i.e. advocating or fundraising for s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Failed suicide = Expensive</td>\n",
              "      <td>kajy93</td>\n",
              "      <td>SuicideWatch</td>\n",
              "      <td>I recently made a suicide attempt. Unfortunate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Being alive is pointless (makes no sense)</td>\n",
              "      <td>kay9t0</td>\n",
              "      <td>SuicideWatch</td>\n",
              "      <td>Let's say you're mentally healthy and not suic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>My cat is the only reason I'm still alive.</td>\n",
              "      <td>kaz14g</td>\n",
              "      <td>SuicideWatch</td>\n",
              "      <td>Let's start saying that I'm Italian so my engl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...                                               body\n",
              "0  New wiki on how to avoid accidentally encourag...  ...  We've been seeing a worrying increase in pro-s...\n",
              "1  Please remember that NO ACTIVISM of any kind i...  ...  Activism, i.e. advocating or fundraising for s...\n",
              "2                         Failed suicide = Expensive  ...  I recently made a suicide attempt. Unfortunate...\n",
              "3          Being alive is pointless (makes no sense)  ...  Let's say you're mentally healthy and not suic...\n",
              "4         My cat is the only reason I'm still alive.  ...  Let's start saying that I'm Italian so my engl...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 396
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp2RWgC89KTA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc4e623-e6d4-449c-caa6-b74b2bd76817"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 397,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3032, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 397
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WmEe6Kb9eSs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35685d8d-5037-4e2b-d5e8-1662751344e8"
      },
      "source": [
        "data['subreddit'].value_counts()"
      ],
      "execution_count": 398,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SuicideWatch    2011\n",
              "depression      1021\n",
              "Name: subreddit, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 398
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9RnB91sVcLS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "3dfae9a8-d9be-4384-8b97-f4a78956ff98"
      },
      "source": [
        "data['body'][0]"
      ],
      "execution_count": 399,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'We\\'ve been seeing a worrying increase in pro-suicide content showing up here and, and also going unreported. This undermines our purpose here, so we wanted to highlight and clarify our guidelines about both direct and indirect incitement of suicide.  \\n\\nWe\\'ve created a wiki that covers these issues.  We hope this will be helpful to anyone who\\'s wondering whether something\\'s okay here and which responses to report.  It explains in detail why *any* validation of suicidal intent, even an \"innocent\" message like \"if you\\'re 100% committed, I\\'ll just wish you peace\" is likely to increase people\\'s pain, and why it\\'s important to report even subtle pro-suicide comments. The full text of the wiki\\'s current version is below, and it is maintained at [/r/SuicideWatch/wiki/incitement](http://www.reddit.com/r/SuicideWatch/wiki/incitement). \\n\\nWe deeply appreciate everyone who gives responsive, empathetic, non-judgemental support to our OPs, and we particularly thank everyone who\\'s already been reporting incitement in all forms.  \\n\\nPlease report any post or comment that encourages suicide (or that breaks any of the other guidelines in the sidebar) to the moderators, either by clicking the \"report\" button or by [sending us a modmail](https://www.reddit.com/message/compose?to=%2Fr%2FSuicideWatch) with a link. We deal with all guideline violations that are reported to us as soon as we can, but we can\\'t read everything so community reports are essential. If you get a PM that breaks the guidelines, please report it both [to the reddit sitewide admins](http://www.reddit.com/report) and to us in modmail. \\n\\nThanks to all the great citizens of the community who help flag problem content and behaviour for us.  \\n    \\n    \\n\\n******\\n***[/r/SuicideWatch/wiki/incitement](http://www.reddit.com/r/SuicideWatch/wiki/incitement)***  \\n*******\\n\\n###Summary###\\n\\n**It\\'s important to respect and understand people\\'s experiences and emotions. It\\'s never necessary, helpful, or kind to support suicidal intent. There are some common misconceptions (discussed below) about suicidal people and how to help them that can cause well-meaning people to inadvertently incite suicide. There are also people online who incite suicide on purpose, often while pretending to be sympathetic and helpful.** \\n\\n###Validate Feelings and Experiences, Not Self-Destructive Intentions###\\n\\nWe\\'re here to offer support, not judgement.  That means accepting, with the best understanding we can offer, whatever emotions people express.  Suicidal people are suffering, and we\\'re here to try to ease that by providing support and caring.  The most reliable way we know to de-escalate someone at risk is to give them the experience of feeling understood. That means not judging whether they should be feeling the way they are, or telling them what to do or not do.  \\n\\nBut there\\'s an important line to draw here.  There\\'s a crucial difference between empathizing with feelings and responding non-judgmentally to suicidal thoughts, and in any way endorsing, encouraging, or validating suicidal intentions or hopeless beliefs. **It\\'s both possible and important to convey understanding and compassion for someone\\'s suicidal thoughts without putting your finger on the scale of their decision.**\\n\\nAnything that condones suicide, even passively, *encourages* suicide. It isn\\'t supportive and does not help. It also violates reddit\\'s sitewide rules as well as our guidelines. Explicitly inciting suicide online is a criminal offense in most jurisdictions. \\n\\nDo not treat any OP\\'s post as meaning that will definitely die by suicide and can\\'t change their minds or be helped. Anyone who\\'s able to read the comments here still has a chance to choose whether or not to try to keep living, even if they\\'ve also been experiencing intense thoughts of suicide, made a suicide plan, or started carrying it out.  \\n\\nIn [the most useful empirical model we have](https://www.apa.org/science/about/psa/2009/06/sci-brief), the desire to die by suicide primarily comes from two interpersonal factors; alienation and a sense of being a burden or having nothing to offer. These factors usually lead to a profound feeling of being unwelcome in the world. \\n\\n**So, any acceptance or reinforcement of suicidal intent, even something \"innocent\" like \"I hope you find peace\", is actually a form of covert shunning that validates a person\\'s sense that they\\'re unwelcome in the world. It will usually add to their pain even if kindly meant and gently worded.**  \\n\\n###How to Avoid Validating Suicidal Intent###\\n\\nKeep the following in mind when offering support to anyone at risk for suicide.  \\n\\n* **People who say they don\\'t want help usually can feel better if they get support that doesn\\'t invalidate their emotions.** Unfortunately, [many popular \"good\" responses are actually counterproductive](https://www.speakingofsuicide.com/2015/03/03/what-not-to-say/). In particular, many friends and family tend to rely exclusively on trying to convince the suicidal person that \"it\\'s not so bad\", and this is usually experienced as \"I don\\'t understand what you\\'re going through and I\\'m not going to try\".  People who\\'ve had \"help\" that made them feel worse don\\'t want any more of the same.  It doesn\\'t mean that someone who actually knows how to be supportive can\\'t give them any comfort.   \\n\\n*  **Most people who are suicidal want to end their** ***pain,*** **not their lives.**  It\\'s almost never true that death is the only way to end these people\\'s suffering. Of course there are exceptional situations, and we certainly acknowledge that, for some people, the right help can be difficult to find. But preventing someone\\'s suicide doesn\\'t mean prolonging their suffering if we do it by giving them real comfort and understanding. \\n\\n* ***An unfixable problem doesn\\'t mean that a good life will never be possible***.  We don\\'t have to fix or change anything to help someone feel better. It\\'s important to keep in mind that the correlation between our outer circumstances and our inner experience is weaker and less direct than commonly assumed.  For every kind of difficult life situation, you will find some people who lapse into suicidal despair, and others who cope amazingly well, and a whole spectrum in between. A key difference is how much inner resilience the person has at the time. This can depend on many personal and situational factors. But when there\\'s not enough, interpersonal support can both compensate for its absence and help rebuild it.  We go into more depth on the \"it gets better\" issue in [this PSA Post](https://www.reddit.com/r/SuicideWatch/comments/25igd7/whats_wrong_with_it_gets_better_what_if_it_doesnt/) which is always linked from our sidebar (community info on mobile) guidelines. \\n\\n* **There are** ***always*** **more choices than brutally forcing someone to stay alive or passively letting them end their lives**.\\n\\nTo avoid accidentally breaking the anti-incitement rule, don\\'t say or try to imply that acting on suicidal thoughts is a good idea, or that someone can\\'t turn back or is already dead.  Do whatever you can to help them feel cared for and welcome, at least in this little corner of the world.  [Our talking tips](http://redd.it/igh87) offer more detailed guidance.\\n\\n###Look Out for Deliberate Incitement.  It May Come in Disguise.###\\n\\nOften comments that subtly encourage suicidal intent actually come from suicide fetishists and voyeurs ([unfortunately this is a real and disturbing phenomenon](https://en.wikipedia.org/wiki/William_Francis_Melchert-Dinkel)). People like this *are* out there and the anonymous nature of reddit makes us particularly attractive to them.  \\n\\nThey will typically try to scratch their psychological \"itch\" by saying things that push people closer to the edge.  They often do this by exploiting the myths that we debunked in the bullet points above.  Specifically you might see people doing the following:\\n\\n* Encouraging the false belief that the only way suicidal people can end their pain is by dying.  **There are** ***always*** **more and better choices than \"brutally forcing someone to stay alive\" or helping (actively or passively) them to end their lives**.\\n\\n* Creating an artificial and toxic sense of \"solidarity\" by linking their encouragement of suicide to empathy.  They will represent themselves as the only one who really understand the suicidal person, while either directly or indirectly encouraging their self-loathing emotions and self-destructive impulses.  **Since most people in suicidal crisis are in desperate need to empathy and understanding, this is a particularly dangerous form of manipulation.**  \\n\\nMany suicide inciters are adept at putting a benevolent spin on their activities while actually luring people away from sources of real help.  A couple of key points to keep in mind: \\n\\n* **Skilled suicide intervention -- peer or professional -- is based on empathic responsiveness to the person\\'s feelings that reduces their suffering in the moment.**  Contrary to pop-culture myths, it does **not** involve persuasion (\"Don\\'t do it!\"), cheerleading (\"You\\'ve got this!\") or meaningless false promises (\"Trust me, it gets better!\"), or invalidation (\"Let me show you how things aren\\'t as bad as you think!\").  Anyone who leads others to expect these kinds of toxic responses, or any other response that prolongs their pain, from expert help may be covertly pro-suicide. (Of course, people sometimes do have bad experience when seeking mental-health treatment, and it\\'s fine to vent about those, but processing our own disappointment and frustration is entirely different from trying to destroy someone else\\'s hope of getting help.)\\n\\n* **Choices made by competent responders are always informed by the understanding that breaching someone\\'s trust is traumatic and must be avoided if possible.**  Any kind of involuntary intervention is an **extremely unlikely** outcome when someone consults a clinician or calls a hotline. (Confidentiality is addressed in more detail in [our Hotlines FAQ post](http://redd.it/1c7ntr)). The goal is always to provide all help with the client\\'s full knowledge and informed consent. We know that no individual or system is perfect.  Mistakes that lead to bad experiences do sometimes happen to vulnerable people, and we have enormous sympathy for them. But anyone who suggests that this is the norm might be trying to scare people away from the help they need.  \\n\\nPlease [let us know discreetly](https://www.reddit.com/message/compose?to=%2Fr%2FSuicideWatch) if you see anyone exhibiting these or similar behaviours. We don\\'t recommend trying to engage with them directly.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 399
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqUjwC2WQt_x",
        "outputId": "130dcf6d-d147-481a-b7dc-2ccb9ffaefde"
      },
      "source": [
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 400,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH4C7cjw-UcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4455490b-8986-4c44-9da0-3d39e9d8e13a"
      },
      "source": [
        "text = \"Hello, Stupid:: idiot!\"\n",
        "def preprocess(string):\n",
        "  string = str(string)\n",
        "  phrase = re.sub(r\"n\\'t\", \" not\", string)\n",
        "  phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "  phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "  phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "  phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "  phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "  phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "  phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "  phrase = re.sub('[^a-z0-9]+', ' ', phrase, flags=re.IGNORECASE)\n",
        "  phrase = re.sub('(\\s+)', ' ', phrase)\n",
        "  phrase = phrase.lower()\n",
        "\n",
        "  text_tokens = word_tokenize(phrase)\n",
        "  return \" \".join([word for word in text_tokens if not word in stopwords.words()])\n",
        "print(preprocess(data['body'][0]))"
      ],
      "execution_count": 401,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seeing worrying increase pro suicide content showing going unreported undermines purpose wanted highlight clarify guidelines direct indirect incitement suicide created wiki covers issues hope helpful anyone wondering whether something okay responses report explains detail validation suicidal intent even innocent message like 100 committed wish peace likely increase people pain important report even subtle pro suicide comments full text wiki current version maintained r suicidewatch wiki incitement http www reddit r suicidewatch wiki incitement deeply appreciate everyone gives responsive empathetic judgemental support ops particularly thank everyone already reporting incitement forms please report post comment encourages suicide breaks guidelines sidebar moderators either clicking report button sending us modmail https www reddit message compose 2fr 2fsuicidewatch link deal guideline violations reported us soon read everything community reports essential get pm breaks guidelines please report reddit sitewide admins http www reddit report us modmail thanks great citizens community help flag problem content behaviour us r suicidewatch wiki incitement http www reddit r suicidewatch wiki incitement summary important respect understand people experiences emotions never necessary helpful kind support suicidal intent common misconceptions discussed suicidal people help cause well meaning people inadvertently incite suicide people online incite suicide purpose often pretending sympathetic helpful validate feelings experiences self destructive intentions offer support judgement means accepting best understanding offer whatever emotions people express suicidal people suffering try ease providing support caring reliable way know escalate someone risk give experience feeling understood means judging whether feeling way telling important line draw crucial difference empathizing feelings responding judgmentally suicidal thoughts way endorsing encouraging validating suicidal intentions hopeless beliefs possible important convey understanding compassion someone suicidal thoughts without putting finger scale decision anything condones suicide even passively encourages suicide supportive help violates reddit sitewide rules well guidelines explicitly inciting suicide online criminal offense jurisdictions treat post meaning definitely suicide change minds helped anyone able read comments still chance choose whether try keep living even experiencing intense thoughts suicide made suicide plan started carrying useful empirical model https www org science psa 2009 06 sci brief desire suicide primarily comes two interpersonal factors alienation sense burden nothing offer factors usually lead profound feeling unwelcome world acceptance reinforcement suicidal intent even something innocent like hope find peace actually form covert shunning validates person sense unwelcome world usually add pain even kindly meant gently worded avoid validating suicidal intent keep following mind offering support anyone risk suicide people say help usually feel better get support invalidate emotions unfortunately many popular good responses actually counterproductive https www speakingofsuicide 2015 03 03 say particular many friends family tend rely exclusively trying convince suicidal person bad usually experienced understand going going try people help made feel worse mean someone actually knows supportive give comfort people suicidal pain lives almost never true death way people suffering course exceptional situations certainly acknowledge people right help difficult find preventing someone suicide mean prolonging suffering giving real comfort understanding unfixable problem mean good life never possible fix change anything help someone feel better important keep mind correlation outer circumstances inner experience weaker less direct commonly assumed every kind difficult life situation find people lapse suicidal despair others cope amazingly well whole spectrum key difference much inner resilience person time depend many personal situational factors enough interpersonal support compensate absence help rebuild go depth gets better issue psa post https www reddit r suicidewatch comments 25igd7 whats wrong gets better doesnt always linked sidebar community info mobile guidelines always choices brutally forcing someone stay alive passively letting lives avoid accidentally breaking anti incitement rule say try imply acting suicidal thoughts good idea someone turn back already dead whatever help feel cared welcome least little corner world talking tips http redd igh87 offer detailed guidance look deliberate incitement may disguise often comments subtly encourage suicidal intent actually suicide fetishists voyeurs unfortunately real disturbing phenomenon https wikipedia org wiki william francis melchert dinkel people like anonymous nature reddit makes us particularly attractive typically try scratch psychological itch saying things push people closer edge often exploiting myths debunked bullet points specifically might see people following encouraging false belief way suicidal people pain dying always better choices brutally forcing someone stay alive helping actively passively lives creating artificial toxic sense solidarity linking encouragement suicide empathy represent really understand suicidal person either directly indirectly encouraging self loathing emotions self destructive impulses since people suicidal crisis desperate need empathy understanding particularly dangerous form manipulation many suicide inciters adept putting benevolent spin activities actually luring people away sources real help couple key points keep mind skilled suicide intervention peer professional based empathic responsiveness person feelings reduces suffering moment contrary pop culture myths involve persuasion cheerleading got meaningless false promises trust gets better invalidation let show things bad think anyone leads others expect kinds toxic responses response prolongs pain expert help may covertly pro suicide course people sometimes bad experience seeking mental health treatment fine vent processing disappointment frustration entirely different trying destroy someone else hope getting help choices made competent responders always informed understanding breaching someone trust traumatic must avoided possible kind involuntary intervention extremely unlikely outcome someone consults clinician calls hotline confidentiality addressed detail hotlines faq post http redd 1c7ntr goal always provide help client full knowledge informed consent know individual system perfect mistakes lead bad experiences sometimes happen vulnerable people enormous sympathy anyone suggests norm might trying scare people away help need please let us know discreetly https www reddit message compose 2fr 2fsuicidewatch see anyone exhibiting similar behaviours recommend trying engage directly\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-oZAD2gRdRK"
      },
      "source": [
        "data['text'] = data['title'] + data['body']\r\n",
        "data['text'] = data['text'].map(lambda string: preprocess(string))"
      ],
      "execution_count": 402,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf25MgqBYzG2"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "import logging\r\n",
        "logging.basicConfig(level=logging.INFO)\r\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 403,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtQHLJRMhfX-"
      },
      "source": [
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
      ],
      "execution_count": 404,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3nx1AhfhpCb",
        "outputId": "00cf0552-4100-48f2-ff8d-37cc13d12cee"
      },
      "source": [
        "!pip install sentencepiece\r\n",
        "!pip install bert-for-tf2"
      ],
      "execution_count": 405,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n",
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.14.7)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.9.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9wDuSy2hkvu",
        "outputId": "4c530aa8-18eb-4363-84e7-8f9a851627bd"
      },
      "source": [
        "import tensorflow_hub as hub\r\n",
        "import bert\r\n",
        "import tokenization\r\n",
        "module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\r\n",
        "bert_layer = hub.KerasLayer(module_url, trainable=True)\r\n"
      ],
      "execution_count": 406,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:resolver HttpCompressedFileResolver does not support the provided handle.\n",
            "INFO:absl:resolver GcsCompressedFileResolver does not support the provided handle.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnV7yhrgV-dh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9e7be6a-9b3c-45c2-c57b-fcaf493ae0d4"
      },
      "source": [
        "X = data['text'].values\n",
        "y = data['subreddit'].values\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "y = le.fit_transform(y)\n",
        "# X = list(X)\n",
        "# y = list(y)\n",
        "print(set(y))\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.02, random_state = 42)\n",
        "print(len(train_X), len(test_X))"
      ],
      "execution_count": 407,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0, 1}\n",
            "2971 61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uNH2_0UhzRv"
      },
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\r\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\r\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\r\n",
        "\r\n",
        "def bert_encode(texts, tokenizer, max_len=512):\r\n",
        "    all_tokens = []\r\n",
        "    all_masks = []\r\n",
        "    all_segments = []\r\n",
        "    \r\n",
        "    for text in texts:\r\n",
        "        text = tokenizer.tokenize(text)\r\n",
        "            \r\n",
        "        text = text[:max_len-2]\r\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\r\n",
        "        pad_len = max_len - len(input_sequence)\r\n",
        "        \r\n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\r\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\r\n",
        "        segment_ids = [0] * max_len\r\n",
        "        \r\n",
        "        all_tokens.append(tokens)\r\n",
        "        all_masks.append(pad_masks)\r\n",
        "        all_segments.append(segment_ids)\r\n",
        "    \r\n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
      ],
      "execution_count": 408,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewFF3fa3iwd2"
      },
      "source": [
        "def build_model(bert_layer, max_len=512):\r\n",
        "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\r\n",
        "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\r\n",
        "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\r\n",
        "\r\n",
        "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\r\n",
        "    clf_output = sequence_output[:, 0, :]\r\n",
        "    net = tf.keras.layers.Dense(32, activation='relu')(clf_output)\r\n",
        "    net = tf.keras.layers.Dropout(0.5)(net)\r\n",
        "    # net = tf.keras.layers.Dense(32, activation='relu')(net)\r\n",
        "    # net = tf.keras.layers.Dropout(0.5)(net)\r\n",
        "    out = tf.keras.layers.Dense(1, activation='sigmoid')(net)\r\n",
        "    \r\n",
        "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\r\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": 409,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_YFGuzWi77P"
      },
      "source": [
        "import keras\r\n",
        "max_len = 100\r\n",
        "train_input = bert_encode(train_X, tokenizer, max_len = max_len)\r\n",
        "test_input = bert_encode(test_X, tokenizer, max_len = max_len)\r\n",
        "train_labels = train_y"
      ],
      "execution_count": 410,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYlvA6SUjYAI",
        "outputId": "063975a5-5324-4df1-a34c-b9104345409a"
      },
      "source": [
        "model = build_model(bert_layer, max_len=max_len)\r\n",
        "model.summary()"
      ],
      "execution_count": 411,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_87\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer_7 (KerasLayer)      [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_44 (T [(None, 768)]        0           keras_layer_7[0][1]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_132 (Dense)               (None, 32)           24608       tf_op_layer_strided_slice_44[0][0\n",
            "__________________________________________________________________________________________________\n",
            "dropout_75 (Dropout)            (None, 32)           0           dense_132[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_133 (Dense)               (None, 1)            33          dropout_75[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 109,506,882\n",
            "Trainable params: 109,506,881\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqU0rp5Uly7g",
        "outputId": "6eae995a-5014-4c41-e8b9-5d993b87bd7c"
      },
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\r\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\r\n",
        "\r\n",
        "train_history = model.fit(\r\n",
        "    train_input, train_labels, \r\n",
        "    validation_split = 0.2,\r\n",
        "    epochs = 5,\r\n",
        "    callbacks = [checkpoint, earlystopping],\r\n",
        "    batch_size = 16,\r\n",
        "    verbose = 1)"
      ],
      "execution_count": 412,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "149/149 [==============================] - ETA: 0s - loss: 0.6468 - accuracy: 0.6397\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.66218, saving model to model.h5\n",
            "149/149 [==============================] - 66s 440ms/step - loss: 0.6468 - accuracy: 0.6397 - val_loss: 0.5999 - val_accuracy: 0.6622\n",
            "Epoch 2/5\n",
            "149/149 [==============================] - ETA: 0s - loss: 0.5580 - accuracy: 0.7256\n",
            "Epoch 00002: val_accuracy improved from 0.66218 to 0.73445, saving model to model.h5\n",
            "149/149 [==============================] - 65s 437ms/step - loss: 0.5580 - accuracy: 0.7256 - val_loss: 0.5339 - val_accuracy: 0.7345\n",
            "Epoch 3/5\n",
            "149/149 [==============================] - ETA: 0s - loss: 0.4765 - accuracy: 0.7891\n",
            "Epoch 00003: val_accuracy did not improve from 0.73445\n",
            "149/149 [==============================] - 59s 394ms/step - loss: 0.4765 - accuracy: 0.7891 - val_loss: 0.5646 - val_accuracy: 0.7008\n",
            "Epoch 4/5\n",
            "149/149 [==============================] - ETA: 0s - loss: 0.3973 - accuracy: 0.8384\n",
            "Epoch 00004: val_accuracy did not improve from 0.73445\n",
            "149/149 [==============================] - 58s 392ms/step - loss: 0.3973 - accuracy: 0.8384 - val_loss: 0.5554 - val_accuracy: 0.7193\n",
            "Epoch 5/5\n",
            "149/149 [==============================] - ETA: 0s - loss: 0.3135 - accuracy: 0.8801\n",
            "Epoch 00005: val_accuracy did not improve from 0.73445\n",
            "149/149 [==============================] - 59s 393ms/step - loss: 0.3135 - accuracy: 0.8801 - val_loss: 0.6964 - val_accuracy: 0.7261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wEL_4vwl7Wp"
      },
      "source": [
        "model.load_weights('model.h5')\r\n",
        "test_pred = model.predict(test_input)"
      ],
      "execution_count": 413,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muj98zx0klCd"
      },
      "source": [
        "def my_round(val):\r\n",
        "  if val >= 0.5:\r\n",
        "    return 1\r\n",
        "  return 0\r\n",
        "test_pred = [my_round(ele) for ele in test_pred]"
      ],
      "execution_count": 414,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IeTKX7ancQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2b0a0f3-caba-4b12-ca1e-dc71d4e2b9ff"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "acc = accuracy_score(test_y, test_pred)\r\n",
        "print (\"Test Set Examples: \", len(test_y)) \r\n",
        "print (\"Test Set Accuracy: \", acc * 100, \"%\")"
      ],
      "execution_count": 415,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Set Examples:  61\n",
            "Test Set Accuracy:  77.04918032786885 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujbznuv2b4kU",
        "outputId": "a5a66baa-ffdc-430b-cc8c-1323e0b1c73c"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from xgboost import XGBClassifier\r\n",
        "clf = Pipeline([('cv', CountVectorizer()), ('xgb', XGBClassifier())])\r\n",
        "\r\n",
        "clf.fit(train_X, train_y)\r\n",
        "print(\"------------Training Done ----------\")\r\n",
        "predictions = clf.predict(test_X)\r\n",
        "\r\n",
        "test_acc_sklearn = np.sum(predictions == test_y) / float(len(test_y)) \r\n",
        "\r\n",
        "\r\n",
        "print (\"Test Set Examples: \", len(test_y)) \r\n",
        "print (\"Test Set Accuracy: \", test_acc_sklearn * 100, \"%\")"
      ],
      "execution_count": 416,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------Training Done ----------\n",
            "Test Set Examples:  61\n",
            "Test Set Accuracy:  68.85245901639344 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4A0Wrgwev6E"
      },
      "source": [
        "vocab_size = 10000\r\n",
        "embedding_dim = 50\r\n",
        "max_length = 100\r\n",
        "oov_tok = '<OOV>'\r\n",
        "trunc_type = 'post'\r\n",
        "\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\r\n",
        "tokenizer.fit_on_texts(train_X)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "sequences = tokenizer.texts_to_sequences(train_X)\r\n",
        "padded = pad_sequences(sequences, maxlen = max_length, truncating = trunc_type)\r\n",
        "\r\n",
        "testing_sequences = tokenizer.texts_to_sequences(test_X)\r\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen = max_length)"
      ],
      "execution_count": 417,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZABqMPQh9SK",
        "outputId": "9e57f04d-5bc4-424e-808f-f7b7f33a1da6"
      },
      "source": [
        "model = tf.keras.Sequential([\r\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\r\n",
        "    tf.keras.layers.Flatten(),\r\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\r\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\r\n",
        "])\r\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\r\n",
        "model.summary()"
      ],
      "execution_count": 418,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (None, 100, 50)           500000    \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 5000)              0         \n",
            "_________________________________________________________________\n",
            "dense_134 (Dense)            (None, 32)                160032    \n",
            "_________________________________________________________________\n",
            "dense_135 (Dense)            (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 660,065\n",
            "Trainable params: 660,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5_g5fxtiBpA",
        "outputId": "99c04e73-f7ca-4f1f-98a3-f305e14b8974"
      },
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model2.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\r\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\r\n",
        "\r\n",
        "num_epochs = 10\r\n",
        "model.fit(padded, train_y, \r\n",
        "          epochs = num_epochs, \r\n",
        "          validation_data = (testing_padded, test_y),\r\n",
        "          callbacks = [checkpoint, earlystopping],\r\n",
        "          batch_size = 16,\r\n",
        "          verbose = 1)"
      ],
      "execution_count": 419,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "184/186 [============================>.] - ETA: 0s - loss: 0.6164 - accuracy: 0.6695\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.65574, saving model to model2.h5\n",
            "186/186 [==============================] - 1s 7ms/step - loss: 0.6152 - accuracy: 0.6705 - val_loss: 0.5754 - val_accuracy: 0.6557\n",
            "Epoch 2/10\n",
            "184/186 [============================>.] - ETA: 0s - loss: 0.3039 - accuracy: 0.8920\n",
            "Epoch 00002: val_accuracy did not improve from 0.65574\n",
            "186/186 [==============================] - 1s 7ms/step - loss: 0.3025 - accuracy: 0.8926 - val_loss: 0.6654 - val_accuracy: 0.6066\n",
            "Epoch 3/10\n",
            "182/186 [============================>.] - ETA: 0s - loss: 0.0792 - accuracy: 0.9797\n",
            "Epoch 00003: val_accuracy improved from 0.65574 to 0.68852, saving model to model2.h5\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0780 - accuracy: 0.9801 - val_loss: 0.6298 - val_accuracy: 0.6885\n",
            "Epoch 4/10\n",
            "181/186 [============================>.] - ETA: 0s - loss: 0.0471 - accuracy: 0.9890\n",
            "Epoch 00004: val_accuracy improved from 0.68852 to 0.70492, saving model to model2.h5\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0473 - accuracy: 0.9886 - val_loss: 0.7013 - val_accuracy: 0.7049\n",
            "Epoch 5/10\n",
            "183/186 [============================>.] - ETA: 0s - loss: 0.0371 - accuracy: 0.9898\n",
            "Epoch 00005: val_accuracy did not improve from 0.70492\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0373 - accuracy: 0.9896 - val_loss: 0.7582 - val_accuracy: 0.6885\n",
            "Epoch 6/10\n",
            "181/186 [============================>.] - ETA: 0s - loss: 0.0337 - accuracy: 0.9896\n",
            "Epoch 00006: val_accuracy did not improve from 0.70492\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0332 - accuracy: 0.9899 - val_loss: 0.8006 - val_accuracy: 0.7049\n",
            "Epoch 7/10\n",
            "180/186 [============================>.] - ETA: 0s - loss: 0.0307 - accuracy: 0.9899\n",
            "Epoch 00007: val_accuracy did not improve from 0.70492\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0301 - accuracy: 0.9902 - val_loss: 0.8680 - val_accuracy: 0.7049\n",
            "Epoch 8/10\n",
            "179/186 [===========================>..] - ETA: 0s - loss: 0.0303 - accuracy: 0.9902\n",
            "Epoch 00008: val_accuracy improved from 0.70492 to 0.72131, saving model to model2.h5\n",
            "186/186 [==============================] - 1s 7ms/step - loss: 0.0306 - accuracy: 0.9902 - val_loss: 0.8695 - val_accuracy: 0.7213\n",
            "Epoch 9/10\n",
            "183/186 [============================>.] - ETA: 0s - loss: 0.0301 - accuracy: 0.9904\n",
            "Epoch 00009: val_accuracy did not improve from 0.72131\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0300 - accuracy: 0.9906 - val_loss: 0.9892 - val_accuracy: 0.7049\n",
            "Epoch 10/10\n",
            "183/186 [============================>.] - ETA: 0s - loss: 0.0287 - accuracy: 0.9911\n",
            "Epoch 00010: val_accuracy did not improve from 0.72131\n",
            "186/186 [==============================] - 1s 6ms/step - loss: 0.0292 - accuracy: 0.9909 - val_loss: 0.9811 - val_accuracy: 0.6721\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f11bee3f080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 419
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75IVxWuRjfeZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}