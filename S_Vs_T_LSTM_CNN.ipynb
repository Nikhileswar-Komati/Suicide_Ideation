{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of V2_XGB_Deployment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNt6Y9O2vzc8qHW0v6ZDGJl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhileswar-Komati/Suicide_Ideation/blob/master/S_Vs_T_LSTM_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTlu-l_bdI1Y"
      },
      "source": [
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
        "!pip install sentencepiece\n",
        "!pip install bert-for-tf2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2DwSquv8qSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c18c31-19e9-4fa6-f3c6-2c7169afdc4b"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from xgboost import XGBClassifier\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os, re\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AXemEX88XIu",
        "outputId": "d9be848a-0303-43f6-f2b8-c39e895d3b12"
      },
      "source": [
        "os.environ['KAGGLE_USERNAME'] = \"nikhileswarkomati\"\n",
        "os.environ['KAGGLE_KEY'] = \"001b3a30170775e55950edb6ff0c9b17\"\n",
        "!kaggle datasets download -d nikhileswarkomati/suicide-watch\n",
        "!unzip '/content/suicide-watch.zip'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading suicide-watch.zip to /content\n",
            " 99% 114M/115M [00:01<00:00, 75.2MB/s]\n",
            "100% 115M/115M [00:01<00:00, 91.7MB/s]\n",
            "Archive:  /content/suicide-watch.zip\n",
            "  inflating: SuicideAndDepression_Detection.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "WqC-R1QJ8gKD",
        "outputId": "93d9a7e1-0835-4cec-f9f8-f7a87cd468b7"
      },
      "source": [
        "data = pd.read_csv('/content/SuicideAndDepression_Detection.csv', lineterminator = '\\n')\n",
        "data.sample(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>284641</th>\n",
              "      <td>For the first time, I saw a serious post witho...</td>\n",
              "      <td>teenagers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294826</th>\n",
              "      <td>My English teacher asked to give my thought ab...</td>\n",
              "      <td>teenagers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200658</th>\n",
              "      <td>Whats r/5050? Title says it all. What is it?\\n...</td>\n",
              "      <td>teenagers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122067</th>\n",
              "      <td>my mum has sent me the most passive aggressive...</td>\n",
              "      <td>teenagers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184336</th>\n",
              "      <td>she didnt leave you on read king you left her ...</td>\n",
              "      <td>teenagers</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text      class\n",
              "284641  For the first time, I saw a serious post witho...  teenagers\n",
              "294826  My English teacher asked to give my thought ab...  teenagers\n",
              "200658  Whats r/5050? Title says it all. What is it?\\n...  teenagers\n",
              "122067  my mum has sent me the most passive aggressive...  teenagers\n",
              "184336  she didnt leave you on read king you left her ...  teenagers"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4_3TFUs8uA-",
        "outputId": "e3f1cbdd-ac04-48e7-86c4-7d947c573466"
      },
      "source": [
        "print(data.shape)\n",
        "data['class'].value_counts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(348111, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "depression      116037\n",
              "teenagers       116037\n",
              "SuicideWatch    116037\n",
              "Name: class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lRhkGN8IE-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76ba6d49-e4ee-4983-b771-d2acf1abd80c"
      },
      "source": [
        "def preprocess(string):\n",
        "    phrase = str(string)\n",
        "    phrase = re.sub('[^a-z]+', ' ', phrase, flags = re.IGNORECASE)\n",
        "    phrase = re.sub('(\\s+)', ' ', phrase)\n",
        "    phrase = re.sub('http\\S+', ' ', phrase)\n",
        "    phrase = phrase.lower()\n",
        "    words_li = ['filler']\n",
        "    li = list(stopwords.words()) + words_li\n",
        "    text_tokens = word_tokenize(phrase)\n",
        "    return \" \".join([word for word in text_tokens if word not in li])\n",
        "print(data['text'].apply(lambda x: len(x.split(' '))).sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59527144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKPzRpE5JzQQ"
      },
      "source": [
        "data['text'] = data['text'].map(lambda string: preprocess(string))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaDIwATpsLRU",
        "outputId": "aa5cb450-58b1-49e2-cf14-52ecfb3020fd"
      },
      "source": [
        "print(data['text'].apply(lambda x: len(x.split(' '))).sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59527144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVEHdqxP671Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a12cd66e-0c72-4424-8d2a-4dfbc89bb336"
      },
      "source": [
        "data.loc[(data['class'] != 'depression'), 'text'][:1000].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txfNhIm4-LEK",
        "outputId": "88ec780f-39b7-4218-f954-14741bb93d30"
      },
      "source": [
        "# X = data.iloc[:, 0].values\n",
        "# y = data.iloc[:, 1].values\n",
        "\n",
        "# le = LabelEncoder()\n",
        "\n",
        "# y = le.fit_transform(y)\n",
        "\n",
        "# train_X, test_X, train_y, test_y = train_test_split(X, y, train_size = 0.5, stratify = y)\n",
        "# train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, train_size = 0.2, stratify = train_y)\n",
        "# print(train_X.shape, test_y.shape, val_X.shape)\n",
        "\n",
        "#------------------------------------------------------------------------------------------\n",
        "X = data.loc[(data['class'] != 'depression'), 'text'].values\n",
        "y = data.loc[(data['class'] != 'depression'), 'class'].values\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, train_size = 0.5, stratify = y)\n",
        "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, train_size = 0.2, stratify = train_y)\n",
        "print(train_X.shape, test_y.shape, val_X.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(23207,) (116037,) (92830,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD4UfZpeexTb",
        "outputId": "26d27c09-2d78-4261-8adb-846092082374"
      },
      "source": [
        "from keras.layers import LSTM, Conv1D, MaxPooling1D\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from keras.layers import LSTM\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import itertools\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Embedding\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import utils\n",
        "\n",
        "\n",
        "embedding_vecor_length = 32\n",
        "vocab_size = 10000\n",
        "max_length = 600\n",
        "oov_tok = '<OOV>'\n",
        "trunc_type = 'post'\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
        "tokenizer.fit_on_texts(train_X)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(train_X)\n",
        "padded = sequence.pad_sequences(sequences, maxlen = max_length, truncating = trunc_type)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_vecor_length, input_length = max_length))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(100, dropout = 0.3, recurrent_dropout = 0.3))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model_lstm.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "callbacks_list = [checkpoint, earlystopping]\n",
        "model.fit(padded, train_y, epochs=10, batch_size=256,verbose = 1,callbacks = callbacks_list,validation_split = 0.1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 600, 32)           320000    \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 600, 32)           3104      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 300, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 376,405\n",
            "Trainable params: 376,405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "82/82 [==============================] - 148s 2s/step - loss: 0.5759 - accuracy: 0.6806 - val_loss: 0.2564 - val_accuracy: 0.9052\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.90521, saving model to model_lstm.h5\n",
            "Epoch 2/10\n",
            "82/82 [==============================] - 145s 2s/step - loss: 0.3651 - accuracy: 0.8739 - val_loss: 0.2402 - val_accuracy: 0.9143\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.90521 to 0.91426, saving model to model_lstm.h5\n",
            "Epoch 3/10\n",
            "82/82 [==============================] - 146s 2s/step - loss: 0.1971 - accuracy: 0.9285 - val_loss: 0.1899 - val_accuracy: 0.9315\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.91426 to 0.93150, saving model to model_lstm.h5\n",
            "Epoch 4/10\n",
            "82/82 [==============================] - 145s 2s/step - loss: 0.1614 - accuracy: 0.9453 - val_loss: 0.1899 - val_accuracy: 0.9311\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.93150\n",
            "Epoch 5/10\n",
            "82/82 [==============================] - 146s 2s/step - loss: 0.1301 - accuracy: 0.9551 - val_loss: 0.2274 - val_accuracy: 0.9173\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.93150\n",
            "Epoch 6/10\n",
            "82/82 [==============================] - 145s 2s/step - loss: 0.1356 - accuracy: 0.9515 - val_loss: 0.2148 - val_accuracy: 0.9311\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.93150\n",
            "Epoch 7/10\n",
            "82/82 [==============================] - 145s 2s/step - loss: 0.1318 - accuracy: 0.9548 - val_loss: 0.2450 - val_accuracy: 0.9112\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.93150\n",
            "Epoch 8/10\n",
            "82/82 [==============================] - 145s 2s/step - loss: 0.5519 - accuracy: 0.8278 - val_loss: 0.5737 - val_accuracy: 0.7411\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.93150\n",
            "Epoch 00008: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6c02b3aa90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdiGvIQJfl63",
        "outputId": "e473c3a2-319d-44ea-9535-df7cde472763"
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model_lstm.h5')\n",
        "testing_sequences = tokenizer.texts_to_sequences(test_X)\n",
        "testing_padded = sequence.pad_sequences(testing_sequences, maxlen = max_length)\n",
        "y_pred = model.predict(testing_padded)\n",
        "print(accuracy_score(np.rint(y_pred), test_y))\n",
        "print(classification_report(np.rint(y_pred), test_y))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9265837620758895\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.91      0.94      0.93     56553\n",
            "         1.0       0.94      0.92      0.93     59484\n",
            "\n",
            "    accuracy                           0.93    116037\n",
            "   macro avg       0.93      0.93      0.93    116037\n",
            "weighted avg       0.93      0.93      0.93    116037\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}