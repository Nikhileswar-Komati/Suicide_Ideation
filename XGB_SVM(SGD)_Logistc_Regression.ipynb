{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "V2_XGB_Deployment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP9A3cpz/AiO34VimMCfI43",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhileswar-Komati/Suicide_Ideation/blob/master/XGB_SVM(SGD)_Logistc_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTlu-l_bdI1Y",
        "outputId": "081bbc0d-cdd0-451a-ebf5-fd0993bb75a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
        "!pip install sentencepiece\n",
        "!pip install bert-for-tf2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 4.3MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/a1/acb891630749c56901e770a34d6bac8a509a367dd74a05daf7306952e910/bert-for-tf2-0.14.9.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.4MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/aa/e0/4f663d8abf83c8084b75b995bd2ab3a9512ebc5b97206fde38cef906ab07/py-params-0.10.2.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-cp37-none-any.whl size=30535 sha256=1776b3404539fdb3afe5b7afa66bb0e2cfe36e981c7816804fb77e0bf47794b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/04/ee/347bd9f5b821b637c76411d280271a857aece00358896a230f\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.2-cp37-none-any.whl size=7912 sha256=69138135989ea2c2977a484b827f6732668978b9f354878e70fc52e62e89bdcc\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/4a/70/ff12450229ff1955abf01f365051d4faae1c20aef53ab4cf09\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp37-none-any.whl size=19472 sha256=e7bdcae2fa0e0d478afa5eca74675119d99ee6367cb869eaf8b67d604f55561a\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2DwSquv8qSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe39cae9-135c-411d-f8e9-935dcec6bff8"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from xgboost import XGBClassifier\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os, re, bert, tokenization\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH98Qwfo8LMj",
        "outputId": "3faa805d-a9d6-4933-da69-1ac9e7d6fc44"
      },
      "source": [
        "os.environ['KAGGLE_USERNAME'] = \"nikhileswarkomati\"\n",
        "os.environ['KAGGLE_KEY'] = \"001b3a30170775e55950edb6ff0c9b17\"\n",
        "!kaggle datasets download -d nikhileswarkomati/suicide-watch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading suicide-watch.zip to /content\n",
            " 96% 111M/115M [00:01<00:00, 96.5MB/s]\n",
            "100% 115M/115M [00:01<00:00, 91.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AXemEX88XIu",
        "outputId": "c1f3cbe6-bba8-4d70-f7bf-4d4e3a574afa"
      },
      "source": [
        "!unzip '/content/suicide-watch.zip'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/suicide-watch.zip\n",
            "  inflating: SuicideAndDepression_Detection.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "WqC-R1QJ8gKD",
        "outputId": "1776a6a8-0b30-4120-bfce-8cbdfb719921"
      },
      "source": [
        "data = pd.read_csv('/content/SuicideAndDepression_Detection.csv', lineterminator = '\\n')\n",
        "data.sample(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>259200</th>\n",
              "      <td>college debt with scholarship vs without schol...</td>\n",
              "      <td>teenagers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106830</th>\n",
              "      <td>I need a late night friend I think. This balon...</td>\n",
              "      <td>depression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214848</th>\n",
              "      <td>SchoolworkIs anyone else just so occupied with...</td>\n",
              "      <td>SuicideWatch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130874</th>\n",
              "      <td>Singing the theme song to The Fresh Prince of ...</td>\n",
              "      <td>depression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83036</th>\n",
              "      <td>dude id hate to be a mod... just doing your jo...</td>\n",
              "      <td>teenagers</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text         class\n",
              "259200  college debt with scholarship vs without schol...     teenagers\n",
              "106830  I need a late night friend I think. This balon...    depression\n",
              "214848  SchoolworkIs anyone else just so occupied with...  SuicideWatch\n",
              "130874  Singing the theme song to The Fresh Prince of ...    depression\n",
              "83036   dude id hate to be a mod... just doing your jo...     teenagers"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4_3TFUs8uA-",
        "outputId": "af49a535-4804-431b-ffe7-c91af296546b"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(348111, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lRhkGN8IE-3",
        "outputId": "62c8a713-d4a5-43ef-860a-228557a867d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def preprocess(string):\n",
        "    phrase = str(string)\n",
        "    phrase = re.sub('[^a-z]+', ' ', phrase, flags = re.IGNORECASE)\n",
        "    phrase = re.sub('(\\s+)', ' ', phrase)\n",
        "    phrase = phrase.lower()\n",
        "    # words_li = ['suicide', 'kill', 'depression', 'fuck', 'filler', 'die', 'depressed']\n",
        "    li = list(stopwords.words()) #+ words_li\n",
        "    text_tokens = word_tokenize(phrase)\n",
        "    return \" \".join([word for word in text_tokens if word not in li])\n",
        "print(data['text'].apply(lambda x: len(x.split(' '))).sum())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59527144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKPzRpE5JzQQ"
      },
      "source": [
        "data['text'] = data['text'].map(lambda string: preprocess(string))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaDIwATpsLRU",
        "outputId": "aa5cb450-58b1-49e2-cf14-52ecfb3020fd"
      },
      "source": [
        "print(data['text'].apply(lambda x: len(x.split(' '))).sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59527144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txfNhIm4-LEK",
        "outputId": "3d3b1138-d093-423e-8ad3-9073bb0d08cb"
      },
      "source": [
        "X = data.iloc[:, 0].values\n",
        "y = data.iloc[:, 1].values\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, train_size = 0.8, stratify = y)\n",
        "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, train_size = 0.3, stratify = train_y)\n",
        "print(train_X.shape, test_y.shape, val_X.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(83546,) (69623,) (194942,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tk3Vx-avQm2",
        "outputId": "f0778952-6873-4aac-efdf-08db52062704"
      },
      "source": [
        "xgb1 = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', XGBClassifier()),\n",
        "              ])\n",
        "\n",
        "xgb1.fit(train_X, train_y)\n",
        "y_pred = xgb1.predict(test_X)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, test_y))\n",
        "print(classification_report(test_y, y_pred))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.7400140758082817\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.65      0.69     23207\n",
            "           1       0.73      0.66      0.69     23208\n",
            "           2       0.77      0.91      0.83     23208\n",
            "\n",
            "    accuracy                           0.74     69623\n",
            "   macro avg       0.74      0.74      0.74     69623\n",
            "weighted avg       0.74      0.74      0.74     69623\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbRXcqp_H90R",
        "outputId": "d382c78c-cb39-4774-c4bc-a7e430f91b17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd2 = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', SGDClassifier()),\n",
        "              ])\n",
        "\n",
        "sgd2.fit(train_X, train_y)\n",
        "y_pred = sgd2.predict(test_X)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, test_y))\n",
        "print(classification_report(test_y, y_pred))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.796389124283642\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.74      0.75     23207\n",
            "           1       0.77      0.71      0.74     23208\n",
            "           2       0.84      0.93      0.89     23208\n",
            "\n",
            "    accuracy                           0.80     69623\n",
            "   macro avg       0.79      0.80      0.79     69623\n",
            "weighted avg       0.79      0.80      0.79     69623\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFc88sTfx11d",
        "outputId": "4e58d458-57ba-40e6-c4d3-bd069be620bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-19 12:59:39--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.75.62\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.75.62|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  69.4MB/s    in 23s     \n",
            "\n",
            "2021-04-19 13:00:02 (67.6 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnBL6z1vxbcD"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "wv = gensim.models.KeyedVectors.load_word2vec_format(\"/content/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
        "wv.init_sims(replace=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-smxKs96ykHw"
      },
      "source": [
        "def word_averaging(wv, words):\n",
        "    all_words, mean = set(), []\n",
        "    \n",
        "    for word in words:\n",
        "        if isinstance(word, np.ndarray):\n",
        "            mean.append(word)\n",
        "        elif word in wv.vocab:\n",
        "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
        "            all_words.add(wv.vocab[word].index)\n",
        "\n",
        "    if not mean:\n",
        "        #logging.warning(\"cannot compute similarity with no input %s\", words)\n",
        "        # FIXME: remove these examples in pre-processing\n",
        "        return np.zeros(wv.vector_size,)\n",
        "\n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
        "    return mean\n",
        "\n",
        "def  word_averaging_list(wv, text_list):\n",
        "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEimCQJYyrct"
      },
      "source": [
        "def w2v_tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text, language='english'):\n",
        "        for word in nltk.word_tokenize(sent, language='english'):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word)\n",
        "    return tokens\n",
        "    \n",
        "for i in range(train_X.shape[0]):\n",
        "    train_X[i] = w2v_tokenize_text(train_X[i])\n",
        "\n",
        "for i in range(test_X.shape[0]):\n",
        "    test_X[i] = w2v_tokenize_text(test_X[i])\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHGRKpJi6qTa",
        "outputId": "5de27ec2-2c12-4892-f269-bdf13813635e"
      },
      "source": [
        "X_train_word_average = word_averaging_list(wv,train_X)\n",
        "X_test_word_average = word_averaging_list(wv,test_X)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGUHNfrn7PbE",
        "outputId": "c81271e3-5b5c-41b2-996e-540f9d1ebc5d"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(n_jobs=1, C=1e5, solver = 'newton-cg')\n",
        "logreg = logreg.fit(X_train_word_average, train_y)\n",
        "y_pred = logreg.predict(X_test_word_average)\n",
        "print('accuracy %s' % accuracy_score(y_pred, test_y))\n",
        "print(classification_report(test_y, y_pred))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.7603665455381124\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.68      0.69     23208\n",
            "           1       0.69      0.75      0.72     23208\n",
            "           2       0.88      0.86      0.87     23207\n",
            "\n",
            "    accuracy                           0.76     69623\n",
            "   macro avg       0.76      0.76      0.76     69623\n",
            "weighted avg       0.76      0.76      0.76     69623\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq-5y0xr_7OM",
        "outputId": "8b5b2a5a-c694-4740-cb7e-4f41dcecf041"
      },
      "source": [
        "import itertools\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import utils\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "\n",
        "max_words = 750\n",
        "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
        "tokenize.fit_on_texts(train_X) # only fit on train\n",
        "\n",
        "x_train = tokenize.texts_to_matrix(train_X)\n",
        "x_test = tokenize.texts_to_matrix(test_X)\n",
        "\n",
        "num_classes = 3\n",
        "y_train = utils.to_categorical(train_y, num_classes)\n",
        "y_test = utils.to_categorical(test_y, num_classes)\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "              \n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    callbacks = [checkpoint, earlystopping],\n",
        "                    validation_split=0.1)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2350/2350 [==============================] - 13s 5ms/step - loss: 0.6424 - accuracy: 0.7253 - val_loss: 0.5312 - val_accuracy: 0.7791\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.77905, saving model to model.h5\n",
            "Epoch 2/10\n",
            "2350/2350 [==============================] - 12s 5ms/step - loss: 0.5070 - accuracy: 0.7874 - val_loss: 0.5239 - val_accuracy: 0.7843\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.77905 to 0.78432, saving model to model.h5\n",
            "Epoch 3/10\n",
            "2350/2350 [==============================] - 12s 5ms/step - loss: 0.4671 - accuracy: 0.8053 - val_loss: 0.5196 - val_accuracy: 0.7820\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.78432\n",
            "Epoch 4/10\n",
            "2350/2350 [==============================] - 13s 5ms/step - loss: 0.4330 - accuracy: 0.8195 - val_loss: 0.5396 - val_accuracy: 0.7750\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.78432\n",
            "Epoch 5/10\n",
            "2350/2350 [==============================] - 13s 5ms/step - loss: 0.3862 - accuracy: 0.8432 - val_loss: 0.5459 - val_accuracy: 0.7771\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.78432\n",
            "Epoch 6/10\n",
            "2350/2350 [==============================] - 13s 5ms/step - loss: 0.3416 - accuracy: 0.8619 - val_loss: 0.5674 - val_accuracy: 0.7797\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.78432\n",
            "Epoch 7/10\n",
            "2350/2350 [==============================] - 12s 5ms/step - loss: 0.3011 - accuracy: 0.8808 - val_loss: 0.5860 - val_accuracy: 0.7728\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.78432\n",
            "Epoch 00007: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eeAo8Z1UKDQ"
      },
      "source": [
        "y_pred = model.predict(x_test)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO4r8J6eUaGD",
        "outputId": "a814f06a-1b8b-46bd-e427-46d11f53f735",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_pred = np.argmax(y_pred, axis = 1)\n",
        "y_test = np.argmax(y_test, axis = 1)\n",
        "print(accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7750456027462189\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.71      0.72     24088\n",
            "           1       0.70      0.75      0.72     21763\n",
            "           2       0.89      0.87      0.88     23772\n",
            "\n",
            "    accuracy                           0.78     69623\n",
            "   macro avg       0.78      0.77      0.77     69623\n",
            "weighted avg       0.78      0.78      0.78     69623\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}