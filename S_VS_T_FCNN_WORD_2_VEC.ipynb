{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S_VS_T_FCNN_WORD_2_VEC",
      "provenance": [],
      "authorship_tag": "ABX9TyPk552cNu+Dvs8av3mJALw/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhileswar-Komati/Suicide_Ideation/blob/master/S_VS_T_FCNN_WORD_2_VEC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2DwSquv8qSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99dd40ca-b71b-4a0b-8999-dffbe06edcb1"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from xgboost import XGBClassifier\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os, re\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH98Qwfo8LMj",
        "outputId": "7f0fed57-5656-4a79-d2ea-f83a2d43816b"
      },
      "source": [
        "os.environ['KAGGLE_USERNAME'] = \"nikhileswarkomati\"\n",
        "os.environ['KAGGLE_KEY'] = \"001b3a30170775e55950edb6ff0c9b17\"\n",
        "!kaggle datasets download -d nikhileswarkomati/suicide-watch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading suicide-watch.zip to /content\n",
            "100% 115M/115M [00:01<00:00, 131MB/s] \n",
            "100% 115M/115M [00:01<00:00, 106MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AXemEX88XIu",
        "outputId": "df937cbd-0b23-4a10-b982-fe7e561d9ce3"
      },
      "source": [
        "!unzip '/content/suicide-watch.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/suicide-watch.zip\n",
            "  inflating: SuicideAndDepression_Detection.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "WqC-R1QJ8gKD",
        "outputId": "ea0c779e-6766-4169-89c1-f194e7c1ddbc"
      },
      "source": [
        "data = pd.read_csv('/content/SuicideAndDepression_Detection.csv', lineterminator = '\\n')\n",
        "data.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>140170</th>\n",
              "      <td>im so depressed and anxious I just can't take ...</td>\n",
              "      <td>depression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333257</th>\n",
              "      <td>I got a Christmas card from my grandma.It talk...</td>\n",
              "      <td>depression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171732</th>\n",
              "      <td>Don't you dare tell me to \"Just snap out of it...</td>\n",
              "      <td>depression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287731</th>\n",
              "      <td>You know... there’s a chance you’ve whacked of...</td>\n",
              "      <td>teenagers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>306716</th>\n",
              "      <td>Bruh I remember the r/teenagers GC on Discord ...</td>\n",
              "      <td>teenagers</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text       class\n",
              "140170  im so depressed and anxious I just can't take ...  depression\n",
              "333257  I got a Christmas card from my grandma.It talk...  depression\n",
              "171732  Don't you dare tell me to \"Just snap out of it...  depression\n",
              "287731  You know... there’s a chance you’ve whacked of...   teenagers\n",
              "306716  Bruh I remember the r/teenagers GC on Discord ...   teenagers"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4_3TFUs8uA-",
        "outputId": "02e0c6b8-ddc6-41b0-e4b7-9ffbc104b2d4"
      },
      "source": [
        "print(data.shape)\n",
        "data['class'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(348111, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "teenagers       116037\n",
              "depression      116037\n",
              "SuicideWatch    116037\n",
              "Name: class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txfNhIm4-LEK",
        "outputId": "1bf88247-9914-42cd-bb90-06a5e79e3b40"
      },
      "source": [
        "# X = data.iloc[:, 0].values\n",
        "# y = data.iloc[:, 1].values\n",
        "\n",
        "# le = LabelEncoder()\n",
        "\n",
        "# y = le.fit_transform(y)\n",
        "\n",
        "# train_X, test_X, train_y, test_y = train_test_split(X, y, train_size = 0.5, stratify = y)\n",
        "# train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, train_size = 0.2, stratify = train_y)\n",
        "# print(train_X.shape, test_y.shape, val_X.shape)\n",
        "\n",
        "#------------------------------------------------------------------------------------------\n",
        "X = data.loc[(data['class'] != 'depression'), 'text'].values\n",
        "y = data.loc[(data['class'] != 'depression'), 'class'].values\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, train_size = 0.5, stratify = y)\n",
        "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, train_size = 0.2, stratify = train_y)\n",
        "print(train_X.shape, test_y.shape, val_X.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(23207,) (116037,) (92830,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4zppob4oOwy"
      },
      "source": [
        "embeddings = embed(data.loc[(data['class'] != 'depression'), 'text'][:10000].values)\n",
        "train_y = le.fit_transform(data.loc[(data['class'] != 'depression'), 'class'][:10000].values)\n",
        "test_embeddings = embed(data.loc[(data['class'] != 'depression'), 'text'][10000:15000].values)\n",
        "test_y = le.transform(data.loc[(data['class'] != 'depression'), 'class'][10000:15000].values)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "# embeddings = embed([preprocess(ele) for ele in data.loc[(data['class'] != 'depression'), 'text'][:10000].values])\n",
        "# train_y = le.fit_transform(data.loc[(data['class'] != 'depression'), 'class'][:10000].values)\n",
        "# test_embeddings = embed([preprocess(ele) for ele in data.loc[(data['class'] != 'depression'), 'text'][10000:15000].values])\n",
        "# test_y = le.transform(data.loc[(data['class'] != 'depression'), 'class'][10000:15000].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PcL0958rwY9",
        "outputId": "4c752789-f689-4bc9-a9f7-f741c5cc36c0"
      },
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-19 08:12:38--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.88.149\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.88.149|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G   110MB/s    in 18s     \n",
            "\n",
            "2021-05-19 08:12:56 (89.7 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1SLrRc4rw5X"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "wv = gensim.models.KeyedVectors.load_word2vec_format(\"/content/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
        "wv.init_sims(replace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAm90lHtrzXx"
      },
      "source": [
        "def word_averaging(wv, words):\n",
        "    all_words, mean = set(), []\n",
        "    \n",
        "    for word in words:\n",
        "        if isinstance(word, np.ndarray):\n",
        "            mean.append(word)\n",
        "        elif word in wv.vocab:\n",
        "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
        "            all_words.add(wv.vocab[word].index)\n",
        "\n",
        "    if not mean:\n",
        "        #logging.warning(\"cannot compute similarity with no input %s\", words)\n",
        "        # FIXME: remove these examples in pre-processing\n",
        "        return np.zeros(wv.vector_size,)\n",
        "\n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
        "    return mean\n",
        "\n",
        "def  word_averaging_list(wv, text_list):\n",
        "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mDV5eO_r5EL"
      },
      "source": [
        "def w2v_tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text, language='english'):\n",
        "        for word in nltk.word_tokenize(sent, language='english'):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word)\n",
        "    return tokens\n",
        "    \n",
        "for i in range(train_X.shape[0]):\n",
        "    train_X[i] = w2v_tokenize_text(train_X[i])\n",
        "\n",
        "for i in range(test_X.shape[0]):\n",
        "    test_X[i] = w2v_tokenize_text(test_X[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FFd0Gq5sCXf",
        "outputId": "7365a289-2e36-4c1c-98cf-55a8f39c7e16"
      },
      "source": [
        "X_train_word_average = word_averaging_list(wv,train_X)\n",
        "X_test_word_average = word_averaging_list(wv,test_X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2h94C4GtuPI",
        "outputId": "246e491a-2a5b-493d-9335-68b5a1aef3fa"
      },
      "source": [
        "import itertools\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Embedding\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import utils\n",
        "from keras.layers import LSTM\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape = (300, )))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "# num_classes = 2\n",
        "# y_train = utils.to_categorical(train_y, num_classes)\n",
        "# y_test = utils.to_categorical(test_y, num_classes)\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 50\n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "history = model.fit(X_train_word_average, train_y,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=50,\n",
        "                    verbose=1,\n",
        "                    callbacks = [checkpoint, earlystopping],\n",
        "                    validation_split=0.1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_32 (Dense)             (None, 512)               154112    \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 1)                 513       \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 154,625\n",
            "Trainable params: 154,625\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "82/82 [==============================] - 1s 12ms/step - loss: 0.5773 - accuracy: 0.7243 - val_loss: 0.3796 - val_accuracy: 0.8414\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.84145, saving model to model.h5\n",
            "Epoch 2/50\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 0.3441 - accuracy: 0.8651 - val_loss: 0.3288 - val_accuracy: 0.8763\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.84145 to 0.87635, saving model to model.h5\n",
            "Epoch 3/50\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 0.3062 - accuracy: 0.8786 - val_loss: 0.3108 - val_accuracy: 0.8798\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.87635 to 0.87979, saving model to model.h5\n",
            "Epoch 4/50\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 0.2851 - accuracy: 0.8882 - val_loss: 0.3040 - val_accuracy: 0.8811\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.87979 to 0.88109, saving model to model.h5\n",
            "Epoch 5/50\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 0.2816 - accuracy: 0.8908 - val_loss: 0.2960 - val_accuracy: 0.8828\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.88109 to 0.88281, saving model to model.h5\n",
            "Epoch 6/50\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 0.2715 - accuracy: 0.8936 - val_loss: 0.2923 - val_accuracy: 0.8837\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.88281 to 0.88367, saving model to model.h5\n",
            "Epoch 7/50\n",
            "82/82 [==============================] - 1s 9ms/step - loss: 0.2648 - accuracy: 0.8949 - val_loss: 0.2903 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.88367 to 0.88496, saving model to model.h5\n",
            "Epoch 8/50\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 0.2564 - accuracy: 0.8986 - val_loss: 0.2830 - val_accuracy: 0.8880\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.88496 to 0.88798, saving model to model.h5\n",
            "Epoch 9/50\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 0.2528 - accuracy: 0.9027 - val_loss: 0.2793 - val_accuracy: 0.8949\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.88798 to 0.89487, saving model to model.h5\n",
            "Epoch 10/50\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 0.2381 - accuracy: 0.9057 - val_loss: 0.2786 - val_accuracy: 0.8893\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.89487\n",
            "Epoch 11/50\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 0.2417 - accuracy: 0.9039 - val_loss: 0.2796 - val_accuracy: 0.8897\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.89487\n",
            "Epoch 12/50\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 0.2427 - accuracy: 0.9067 - val_loss: 0.2759 - val_accuracy: 0.8906\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.89487\n",
            "Epoch 13/50\n",
            "82/82 [==============================] - 1s 9ms/step - loss: 0.2303 - accuracy: 0.9098 - val_loss: 0.2683 - val_accuracy: 0.8979\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.89487 to 0.89789, saving model to model.h5\n",
            "Epoch 14/50\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 0.2274 - accuracy: 0.9104 - val_loss: 0.2677 - val_accuracy: 0.8940\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.89789\n",
            "Epoch 15/50\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 0.2210 - accuracy: 0.9132 - val_loss: 0.2678 - val_accuracy: 0.8931\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.89789\n",
            "Epoch 16/50\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 0.2283 - accuracy: 0.9121 - val_loss: 0.2701 - val_accuracy: 0.8940\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.89789\n",
            "Epoch 17/50\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 0.2165 - accuracy: 0.9140 - val_loss: 0.2704 - val_accuracy: 0.8949\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.89789\n",
            "Epoch 18/50\n",
            "82/82 [==============================] - 1s 9ms/step - loss: 0.2195 - accuracy: 0.9148 - val_loss: 0.2649 - val_accuracy: 0.8962\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.89789\n",
            "Epoch 00018: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T51DEkl_uknH",
        "outputId": "58ffaa1e-2819-4233-f521-b958ddc04e7f"
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/model.h5')\n",
        "y_pred = model.predict(X_test_word_average)\n",
        "y_pred = np.rint(y_pred)\n",
        "y_test = test_y\n",
        "print(accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_pred, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9077190896007308\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.89      0.91     60141\n",
            "         1.0       0.89      0.92      0.91     55896\n",
            "\n",
            "    accuracy                           0.91    116037\n",
            "   macro avg       0.91      0.91      0.91    116037\n",
            "weighted avg       0.91      0.91      0.91    116037\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}